{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoETH\n",
      "shapes: destinations=(157, 12) autopool_destinations=(55, 3) destination_states=(298568, 17) plans=(1623, 28) events=(649, 14) mainnet_blocks=(139820, 3) limited_events_df=(649, 41)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'destination_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mainnet-launch-FtycU18g-py3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'destination_out'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 270\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(autopool\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes: destinations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestinations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m autopool_destinations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautopool_destinations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m destination_states=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m plans=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplans\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m events=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevents\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mainnet_blocks=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmainnet_blocks\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m limited_events_df=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimited_events_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m full_df \u001b[38;5;241m=\u001b[39m \u001b[43madd_virtual_price_values_no_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautopool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimited_events_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m df \u001b[38;5;241m=\u001b[39m full_df[full_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[1;32m    272\u001b[0m timestamp_str \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 250\u001b[0m, in \u001b[0;36madd_virtual_price_values_no_threads\u001b[0;34m(autopool, limited_events_df)\u001b[0m\n\u001b[1;32m    246\u001b[0m new_rows \u001b[38;5;241m=\u001b[39m limited_events_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: determine_forward_looking_vp(autopool, row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    247\u001b[0m all_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(new_rows)\n\u001b[1;32m    249\u001b[0m all_results_df\u001b[38;5;241m.\u001b[39mloc[\n\u001b[0;32m--> 250\u001b[0m     \u001b[43mall_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdestination_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m autopool\u001b[38;5;241m.\u001b[39mautopool_eth_addr,\n\u001b[1;32m    251\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_30_day_fee_and_base_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_60_day_fee_and_base_out\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    252\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    253\u001b[0m all_results_df\u001b[38;5;241m.\u001b[39mloc[\n\u001b[1;32m    254\u001b[0m     all_results_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestination_in\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m autopool\u001b[38;5;241m.\u001b[39mautopool_eth_addr,\n\u001b[1;32m    255\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_30_day_fee_and_base_in\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_60_day_fee_and_base_in\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    256\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_results_df\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mainnet-launch-FtycU18g-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mainnet-launch-FtycU18g-py3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'destination_out'"
     ]
    }
   ],
   "source": [
    "from mainnet_launch.data_fetching.get_state_by_block import (\n",
    "    get_state_by_one_block,\n",
    "    build_blocks_to_use,\n",
    "    get_raw_state_by_blocks,\n",
    "    safe_normalize_6_with_bool_success,\n",
    "    safe_normalize_with_bool_success,\n",
    ")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures as cf\n",
    "\n",
    "from mainnet_launch.database.postgres_operations import (\n",
    "    get_full_table_as_df_with_block,\n",
    "    get_full_table_as_df,\n",
    "    get_full_table_as_df_with_tx_hash,\n",
    ")\n",
    "from mainnet_launch.database.schema.full import (\n",
    "    DestinationStates,\n",
    "    Destinations,\n",
    "    AutopoolDestinations,\n",
    "    RebalanceEvents,\n",
    "    RebalancePlans,\n",
    "    Blocks,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "from multicall import Call\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from mainnet_launch.data_fetching.defi_llama.fetch_timestamp import fetch_blocks_by_unix_timestamps_defillama\n",
    "\n",
    "from mainnet_launch.constants import (\n",
    "    AUTO_USD,\n",
    "    ETH_CHAIN,\n",
    "    AutopoolConstants,\n",
    "    AUTO_DOLA,\n",
    "    BASE_USD,\n",
    "    BASE_CHAIN,\n",
    "    ALL_AUTOPOOLS,\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = None\n",
    "\n",
    "\n",
    "def _extract_limited_events_data(\n",
    "    autopool: AutopoolConstants,\n",
    "    events: pd.DataFrame,\n",
    "    plans: pd.DataFrame,\n",
    "    destination_states: pd.DataFrame,\n",
    "    destinations: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    limited_events_df = events[\n",
    "        [\"destination_in\", \"destination_out\", \"block\", \"safe_value_out\", \"rebalance_file_path\"]\n",
    "    ].copy()\n",
    "\n",
    "    get_fee_and_base_apr = destination_states.set_index([\"destination_vault_address\", \"rebalance_plan_key\"])[\n",
    "        \"fee_plus_base_apr\"\n",
    "    ].to_dict()\n",
    "\n",
    "    limited_events_df[\"fee_and_base_out\"] = limited_events_df.apply(\n",
    "        lambda row: get_fee_and_base_apr.get((row[\"destination_out\"], row[\"rebalance_file_path\"]), None), axis=1\n",
    "    )\n",
    "    limited_events_df[\"fee_and_base_in\"] = limited_events_df.apply(\n",
    "        lambda row: get_fee_and_base_apr.get((row[\"destination_in\"], row[\"rebalance_file_path\"]), None), axis=1\n",
    "    )\n",
    "\n",
    "    destination_names = destinations.set_index(\"destination_vault_address\")[\"underlying_name\"].to_dict()\n",
    "    exchange_names = destinations.set_index(\"destination_vault_address\")[\"exchange_name\"].to_dict()\n",
    "    pool_addresses = destinations.set_index(\"destination_vault_address\")[\"pool\"].to_dict()\n",
    "\n",
    "    limited_events_df[\"destination_in_name\"] = limited_events_df[\"destination_in\"].map(destination_names)\n",
    "    limited_events_df[\"destination_out_name\"] = limited_events_df[\"destination_out\"].map(destination_names)\n",
    "    limited_events_df[\"out_exchange_name\"] = limited_events_df[\"destination_out\"].map(exchange_names)\n",
    "    limited_events_df[\"in_exchange_name\"] = limited_events_df[\"destination_in\"].map(exchange_names)\n",
    "    limited_events_df[\"pool_in\"] = limited_events_df[\"destination_in\"].map(pool_addresses)\n",
    "    limited_events_df[\"pool_out\"] = limited_events_df[\"destination_out\"].map(pool_addresses)\n",
    "\n",
    "    # Join limited_events_df with plans on rebalance_file_path = file_name\n",
    "    limited_events_df = limited_events_df.merge(plans, left_on=\"rebalance_file_path\", right_on=\"file_name\", how=\"left\")\n",
    "\n",
    "    return limited_events_df\n",
    "\n",
    "\n",
    "def load_data(autopool: AutopoolConstants):\n",
    "    destinations = get_full_table_as_df(Destinations, where_clause=Destinations.chain_id == autopool.chain.chain_id)\n",
    "    autopool_destinations = get_full_table_as_df(\n",
    "        AutopoolDestinations, where_clause=AutopoolDestinations.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    # 2 min to fetch\n",
    "    destination_states = get_full_table_as_df_with_block(\n",
    "        DestinationStates,\n",
    "        where_clause=DestinationStates.destination_vault_address.in_(\n",
    "            destinations[\"destination_vault_address\"].tolist()\n",
    "        ),\n",
    "    )\n",
    "    plans = get_full_table_as_df(\n",
    "        RebalancePlans, where_clause=RebalancePlans.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    events = get_full_table_as_df_with_tx_hash(\n",
    "        RebalanceEvents, where_clause=RebalanceEvents.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    mainnet_blocks = get_full_table_as_df(Blocks, where_clause=Blocks.chain_id == autopool.chain.chain_id).sort_values(\n",
    "        \"block\"\n",
    "    )\n",
    "\n",
    "    limited_events_df = _extract_limited_events_data(autopool, events, plans, destination_states, destinations)\n",
    "    return destinations, autopool_destinations, destination_states, plans, events, mainnet_blocks, limited_events_df\n",
    "\n",
    "\n",
    "VP_METHODS = [\n",
    "    (\"getRate\", [\"getRate()(uint256)\"], None),\n",
    "    (\"get_virtual_price\", [\"get_virtual_price()(uint256)\"], None),\n",
    "    (\"convertToAssets_1e18\", [\"convertToAssets(uint256)(uint256)\", int(10**18)], int(10**18)),\n",
    "    (\"stEthPerToken\", [\"stEthPerToken()(uint256)\"], None),\n",
    "]\n",
    "\n",
    "\n",
    "def build_vp_calls(pool_address: str):\n",
    "    calls = []\n",
    "    for suffix, fn, _ in VP_METHODS:\n",
    "        key = f\"{pool_address}:{suffix}\"\n",
    "        calls.append(\n",
    "            Call(\n",
    "                target=pool_address,\n",
    "                function=fn,\n",
    "                returns=[(key, safe_normalize_with_bool_success)],\n",
    "            )\n",
    "        )\n",
    "    return calls\n",
    "\n",
    "\n",
    "def _get_working_virtual_price_column(df: pd.DataFrame, cols_in_priority: list[str]) -> pd.Series:\n",
    "    for col in cols_in_priority:\n",
    "        if not any(df[col].isna()):\n",
    "            return df[col]\n",
    "    print(df[cols_in_priority])\n",
    "\n",
    "    raise ValueError(\"could not identify working virtual price column\")\n",
    "\n",
    "\n",
    "def compute_apr(vp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    t0 = vp_df.index[0]\n",
    "    days = (vp_df.index - t0).total_seconds() / 86400.0\n",
    "\n",
    "    out0 = vp_df[\"out_vp\"].iloc[0]\n",
    "    in0 = vp_df[\"in_vp\"].iloc[0]\n",
    "\n",
    "    # annualized % using actual elapsed days; guard day=0 at start\n",
    "    vp_df[\"out_ann_pct\"] = np.where(days > 0, ((vp_df[\"out_vp\"] / out0) ** (365.0 / days) - 1.0), np.nan)\n",
    "    vp_df[\"in_ann_pct\"] = np.where(days > 0, ((vp_df[\"in_vp\"] / in0) ** (365.0 / days) - 1.0), np.nan)\n",
    "\n",
    "    return vp_df[[\"block\", \"out_vp\", \"in_vp\", \"out_ann_pct\", \"in_ann_pct\"]]\n",
    "\n",
    "\n",
    "def _fetch_vp_df(blocks_to_query: list[int], row: pd.Series, autopool: AutopoolConstants) -> pd.DataFrame:\n",
    "    out_addr = row[\"pool_out\"]\n",
    "    in_addr = row[\"pool_in\"]\n",
    "\n",
    "    calls_to_make = []\n",
    "    calls_to_make += build_vp_calls(out_addr)\n",
    "    calls_to_make += build_vp_calls(in_addr)\n",
    "\n",
    "    vp_df = get_raw_state_by_blocks(\n",
    "        calls_to_make,\n",
    "        blocks_to_query,\n",
    "        autopool.chain,\n",
    "        include_block_number=True,\n",
    "    )\n",
    "\n",
    "    # Coalesce per destination in the same priority order as VP_METHODS\n",
    "    out_cols = [f\"{out_addr}:{suffix}\" for suffix, _, _ in VP_METHODS]\n",
    "    in_cols = [f\"{in_addr}:{suffix}\" for suffix, _, _ in VP_METHODS]\n",
    "\n",
    "    vp_df[\"out_vp\"] = _get_working_virtual_price_column(vp_df, out_cols)\n",
    "    vp_df[\"in_vp\"] = _get_working_virtual_price_column(vp_df, in_cols)\n",
    "\n",
    "    apr_df = compute_apr(vp_df)\n",
    "    return apr_df\n",
    "\n",
    "\n",
    "def determine_forward_looking_vp(autopool: AutopoolConstants, row: pd.Series):\n",
    "    try:\n",
    "        start_block = int(row[\"block\"])\n",
    "        chain_to_approx_blocks_per_day = {\n",
    "            ETH_CHAIN: 7150,  # crude approx\n",
    "            BASE_CHAIN: 43200,  # crude approx\n",
    "        }\n",
    "        approx_blocks_per_day = chain_to_approx_blocks_per_day[autopool.chain]\n",
    "\n",
    "        block_30_days = start_block + (approx_blocks_per_day * 30)\n",
    "        block_60_days = start_block + (approx_blocks_per_day * 60)\n",
    "\n",
    "        today_block = autopool.chain.get_block_near_top()\n",
    "\n",
    "        if (block_30_days > today_block) or (block_60_days > today_block):\n",
    "            return {\n",
    "                **row,\n",
    "                \"valid\": False,\n",
    "            }\n",
    "\n",
    "        blocks_to_query = [start_block, block_30_days, block_60_days]\n",
    "\n",
    "        apr_df = _fetch_vp_df(blocks_to_query, row, autopool)\n",
    "\n",
    "        actual_30_day_fee_and_base_out = apr_df.loc[apr_df[\"block\"] == block_30_days, \"out_ann_pct\"].values[0]\n",
    "        actual_60_day_fee_and_base_out = apr_df.loc[apr_df[\"block\"] == block_60_days, \"out_ann_pct\"].values[0]\n",
    "\n",
    "        actual_30_day_fee_and_base_in = apr_df.loc[apr_df[\"block\"] == block_30_days, \"in_ann_pct\"].values[0]\n",
    "        actual_60_day_fee_and_base_in = apr_df.loc[apr_df[\"block\"] == block_60_days, \"in_ann_pct\"].values[0]\n",
    "\n",
    "        block_timstamp_30_days = apr_df[\"block\"].loc[apr_df[\"block\"] == block_30_days].index[0]\n",
    "        block_timstamp_60_days = apr_df[\"block\"].loc[apr_df[\"block\"] == block_60_days].index[0]\n",
    "\n",
    "        return {\n",
    "            **row,\n",
    "            \"actual_30_day_fee_and_base_out\": actual_30_day_fee_and_base_out,\n",
    "            \"actual_60_day_fee_and_base_out\": actual_60_day_fee_and_base_out,\n",
    "            \"actual_30_day_fee_and_base_in\": actual_30_day_fee_and_base_in,\n",
    "            \"actual_60_day_fee_and_base_in\": actual_60_day_fee_and_base_in,\n",
    "            \"start_block\": start_block,\n",
    "            \"block_30_days\": block_30_days,\n",
    "            \"block_60_days\": block_60_days,\n",
    "            \"timestamp_30_days\": block_timstamp_30_days,\n",
    "            \"timestamp_60_days\": block_timstamp_60_days,\n",
    "            \"valid\": True,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row with block {row['block']}: {e}\")\n",
    "        return {\n",
    "            **row,\n",
    "            \"valid\": False,\n",
    "            \"error\": str(e),\n",
    "            \"error_type\": type(e).__name__,\n",
    "        }\n",
    "\n",
    "\n",
    "def add_virtual_price_values_no_threads(autopool: AutopoolConstants, limited_events_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_rows = limited_events_df.apply(lambda row: determine_forward_looking_vp(autopool, row), axis=1).tolist()\n",
    "    all_results_df = pd.DataFrame.from_records(new_rows)\n",
    "\n",
    "    all_results_df.loc[\n",
    "        all_results_df[\"destination_out\"] == autopool.autopool_eth_addr,\n",
    "        [\"actual_30_day_fee_and_base_out\", \"actual_60_day_fee_and_base_out\"],\n",
    "    ] = 0\n",
    "    all_results_df.loc[\n",
    "        all_results_df[\"destination_in\"] == autopool.autopool_eth_addr,\n",
    "        [\"actual_30_day_fee_and_base_in\", \"actual_60_day_fee_and_base_in\"],\n",
    "    ] = 0\n",
    "\n",
    "    return all_results_df\n",
    "\n",
    "\n",
    "for autopool in ALL_AUTOPOOLS:\n",
    "    if autopool.chain in [ETH_CHAIN, BASE_CHAIN]:\n",
    "        destinations, autopool_destinations, destination_states, plans, events, mainnet_blocks, limited_events_df = (\n",
    "            load_data(autopool)\n",
    "        )\n",
    "        print(autopool.name)\n",
    "        print(\n",
    "            f\"shapes: destinations={destinations.shape} autopool_destinations={autopool_destinations.shape} destination_states={destination_states.shape} plans={plans.shape} events={events.shape} mainnet_blocks={mainnet_blocks.shape} limited_events_df={limited_events_df.shape}\"\n",
    "        )\n",
    "        full_df = add_virtual_price_values_no_threads(autopool, limited_events_df.reset_index())\n",
    "        df = full_df[full_df[\"valid\"] == True]\n",
    "        timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{autopool.name}_fee_and_base_apr_{timestamp_str}.csv\"\n",
    "        full_df.to_csv(filename)\n",
    "        print(full_df[\"valid\"].value_counts())\n",
    "        print(f\"Loaded {full_df.shape} rebalance events for {autopool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby(\"destination_out_name\")[\"valid\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for both 30-day and 60-day comparisons\n",
    "for period in [\"30_day\", \"60_day\"]:\n",
    "    # Plot for \"in\" destinations\n",
    "    fig_in = px.scatter(\n",
    "        df,\n",
    "        x=f\"fee_and_base_in\",\n",
    "        y=f\"actual_{period}_fee_and_base_in\",\n",
    "        color=\"destination_in_name\",\n",
    "        title=f\"Expected vs Actual {period.replace('_', '-').title()} Fee+Base APR (In)\",\n",
    "    )\n",
    "    fig_in.add_trace(\n",
    "        px.line(x=[0, 0.1], y=[0, 0.1]).data[0].update(line=dict(dash=\"dash\", color=\"gray\"), showlegend=False)\n",
    "    )\n",
    "    fig_in.show()\n",
    "    continue\n",
    "\n",
    "    # Plot for \"out\" destinations (only if fee_and_base_out exists)\n",
    "    df_with_out = df.dropna(subset=[\"fee_and_base_out\"])\n",
    "    if len(df_with_out) > 0:\n",
    "        fig_out = px.scatter(\n",
    "            df_with_out,\n",
    "            x=f\"fee_and_base_out\",\n",
    "            y=f\"actual_{period}_fee_and_base_out\",\n",
    "            color=\"destination_out_name\",\n",
    "            title=f\"Expected vs Actual {period.replace('_', '-').title()} Fee+Base APR (Out)\",\n",
    "        )\n",
    "        fig_out.add_trace(\n",
    "            px.line(x=[0, 0.1], y=[0, 0.1]).data[0].update(line=dict(dash=\"dash\", color=\"gray\"), showlegend=False)\n",
    "        )\n",
    "        fig_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one limitation could be, we can only predict at the .1% level, no way we are accurate closer than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Determine common x-axis range\n",
    "x_min = min(all_results_df[\"actual_30_day_fee_and_base_in\"].min(), all_results_df[\"fee_and_base_in\"].min())\n",
    "x_max = max(all_results_df[\"actual_30_day_fee_and_base_in\"].max(), all_results_df[\"fee_and_base_in\"].max())\n",
    "\n",
    "# Create subplots with 2 rows and 1 column\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Distribution of actual fee + base APR we enter\",\n",
    "        \"Distribution of expected fee + base APR we enter\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create histograms\n",
    "fig1 = px.histogram(all_results_df, x=\"actual_30_day_fee_and_base_in\")\n",
    "fig2 = px.histogram(all_results_df, x=\"fee_and_base_in\")\n",
    "\n",
    "# Add traces\n",
    "for trace in fig1.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in fig2.data:\n",
    "    fig.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# Update x-axes to have the same range\n",
    "fig.update_xaxes(range=[x_min, x_max], row=1, col=1)\n",
    "fig.update_xaxes(range=[x_min, x_max], row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=700, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = all_results_df.copy().dropna(subset=[\"actual_30_day_fee_and_base_in\"])\n",
    "sub_df[\"difference\"] = sub_df[\"actual_30_day_fee_and_base_in\"] - sub_df[\"fee_and_base_in\"]\n",
    "sub_df = sub_df.sort_values(\"difference\", ascending=False)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.ecdf(\n",
    "    all_results_df.dropna(subset=[\"actual_30_day_fee_and_base_in\"]),\n",
    "    x=[\"actual_30_day_fee_and_base_in\", \"actual_60_day_fee_and_base_in\", \"fee_and_base_in\"],\n",
    "    title=\"ECDF of actual fee + base APR we enter\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip to 5%, prevent the worst outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Expected Fee+Base APR (In)\", \"Actual 30-Day Fee+Base APR (In)\"))\n",
    "\n",
    "# ECDF for expected (fee_and_base_in)\n",
    "plot_df_expected = all_results_df.dropna(subset=[\"fee_and_base_in\"]).copy()\n",
    "fig_expected = px.ecdf(\n",
    "    plot_df_expected,\n",
    "    x=\"fee_and_base_in\",\n",
    ")\n",
    "for trace in fig_expected.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# ECDF for actual 30-day (actual_30_day_fee_and_base_in)\n",
    "plot_df_actual = all_results_df.dropna(subset=[\"actual_30_day_fee_and_base_in\"]).copy()\n",
    "fig_actual = px.ecdf(plot_df_actual, x=\"actual_30_day_fee_and_base_in\")\n",
    "for trace in fig_actual.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title_text=\"Expected vs Actual Fee+Base APR Distribution (In)\", height=500, showlegend=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df[\"30_day_out_diff\"] = (\n",
    "    all_results_df[\"actual_30_day_fee_and_base_out\"] - all_results_df[\"fee_and_base_out\"]\n",
    ")\n",
    "all_results_df[\"60_day_out_diff\"] = (\n",
    "    all_results_df[\"actual_60_day_fee_and_base_out\"] - all_results_df[\"fee_and_base_out\"]\n",
    ")\n",
    "all_results_df[\"30_day_in_diff\"] = all_results_df[\"actual_30_day_fee_and_base_in\"] - all_results_df[\"fee_and_base_in\"]\n",
    "all_results_df[\"60_day_in_diff\"] = all_results_df[\"actual_60_day_fee_and_base_in\"] - all_results_df[\"fee_and_base_in\"]\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "plot_df = all_results_df.dropna(subset=[\"destination_out_name\", \"30_day_out_diff\"]).copy()\n",
    "\n",
    "fig = px.ecdf(\n",
    "    plot_df,\n",
    "    x=\"30_day_out_diff\",\n",
    "    color=\"destination_out_name\",  # one ECDF line per destination\n",
    "    title=\"Difference between Actual and Planned 30-Day Fee+Base APR (Out)\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"Destination (out)\",\n",
    ")\n",
    "print(\"diff >0 means actual > expected\")\n",
    "print(\"diff <0 means actual < expected\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = all_results_df.dropna(subset=[\"destination_in_name\", \"30_day_in_diff\"]).copy()\n",
    "\n",
    "fig = px.ecdf(\n",
    "    plot_df,\n",
    "    x=\"30_day_in_diff\",\n",
    "    color=\"destination_in_name\",\n",
    "    title=\"Difference between Actual and Planned 30-Day Fee+Base APR (In)\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"Destination (in)\",\n",
    ")\n",
    "print(\"diff >0 means actual > expected\")\n",
    "print(\"diff <0 means actual < expected\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should be doing the lowest hanging fruit first? like the ones that are way off?\n",
    "# Calculate absolute differences for sorting\n",
    "all_results_df[\"abs_30_day_out_diff\"] = all_results_df[\"30_day_out_diff\"].abs()\n",
    "all_results_df[\"abs_60_day_out_diff\"] = all_results_df[\"60_day_out_diff\"].abs()\n",
    "all_results_df[\"abs_30_day_in_diff\"] = all_results_df[\"30_day_in_diff\"].abs()\n",
    "all_results_df[\"abs_60_day_in_diff\"] = all_results_df[\"60_day_in_diff\"].abs()\n",
    "\n",
    "# Find the most off predictions for each category\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORST PREDICTIONS (60-day OUT)\")\n",
    "print(\"=\" * 80)\n",
    "all_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols = [\n",
    "    \"destination_in_name\",\n",
    "    \"destination_out_name\",\n",
    "    \"actual_30_day_fee_and_base_out\",\n",
    "    \"actual_60_day_fee_and_base_out\",\n",
    "    \"actual_30_day_fee_and_base_in\",\n",
    "    \"actual_60_day_fee_and_base_in\",\n",
    "    \"fee_and_base_out\",\n",
    "    \"fee_and_base_in\",\n",
    "    \"30_day_out_diff\",\n",
    "    \"60_day_out_diff\",\n",
    "    \"30_day_in_diff\",\n",
    "    \"60_day_in_diff\",\n",
    "    \"abs_30_day_out_diff\",\n",
    "    \"block\",\n",
    "    \"safe_value_out\",\n",
    "]\n",
    "\n",
    "all_results_df.sort_values(by=\"abs_60_day_out_diff\", ascending=False)[interesting_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainnet-launch-FtycU18g-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
