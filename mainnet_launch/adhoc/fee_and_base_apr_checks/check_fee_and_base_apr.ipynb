{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pb/Library/Caches/pypoetry/virtualenvs/mainnet-launch-FtycU18g-py3.10/lib/python3.10/site-packages/eth_abi/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from mainnet_launch.data_fetching.get_state_by_block import (\n",
    "    get_state_by_one_block,\n",
    "    build_blocks_to_use,\n",
    "    get_raw_state_by_blocks,\n",
    "    safe_normalize_6_with_bool_success,\n",
    "    safe_normalize_with_bool_success,\n",
    ")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures as cf\n",
    "\n",
    "from mainnet_launch.database.postgres_operations import (\n",
    "    get_full_table_as_df_with_block,\n",
    "    get_full_table_as_df,\n",
    "    get_full_table_as_df_with_tx_hash,\n",
    ")\n",
    "from mainnet_launch.database.schema.full import (\n",
    "    DestinationStates,\n",
    "    Destinations,\n",
    "    AutopoolDestinations,\n",
    "    RebalanceEvents,\n",
    "    RebalancePlans,\n",
    "    Blocks,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "from multicall import Call\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from mainnet_launch.data_fetching.defi_llama.fetch_timestamp import fetch_blocks_by_unix_timestamps_defillama\n",
    "\n",
    "from mainnet_launch.constants import (\n",
    "    AUTO_USD,\n",
    "    ETH_CHAIN,\n",
    "    AutopoolConstants,\n",
    "    AUTO_DOLA,\n",
    "    BASE_USD,\n",
    "    BASE_CHAIN,\n",
    "    ALL_AUTOPOOLS,\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = None\n",
    "\n",
    "\n",
    "def _extract_limited_events_data(\n",
    "    autopool: AutopoolConstants,\n",
    "    events: pd.DataFrame,\n",
    "    plans: pd.DataFrame,\n",
    "    destination_states: pd.DataFrame,\n",
    "    destinations: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    limited_events_df = events[\n",
    "        [\"destination_in\", \"destination_out\", \"block\", \"safe_value_out\", \"rebalance_file_path\"]\n",
    "    ].copy()\n",
    "\n",
    "    get_fee_and_base_apr = destination_states.set_index([\"destination_vault_address\", \"rebalance_plan_key\"])[\n",
    "        \"fee_plus_base_apr\"\n",
    "    ].to_dict()\n",
    "\n",
    "    limited_events_df[\"fee_and_base_out\"] = limited_events_df.apply(\n",
    "        lambda row: get_fee_and_base_apr.get((row[\"destination_out\"], row[\"rebalance_file_path\"]), None), axis=1\n",
    "    )\n",
    "    limited_events_df[\"fee_and_base_in\"] = limited_events_df.apply(\n",
    "        lambda row: get_fee_and_base_apr.get((row[\"destination_in\"], row[\"rebalance_file_path\"]), None), axis=1\n",
    "    )\n",
    "\n",
    "    destination_names = destinations.set_index(\"destination_vault_address\")[\"underlying_name\"].to_dict()\n",
    "    exchange_names = destinations.set_index(\"destination_vault_address\")[\"exchange_name\"].to_dict()\n",
    "    pool_addresses = destinations.set_index(\"destination_vault_address\")[\"pool\"].to_dict()\n",
    "\n",
    "    limited_events_df[\"destination_in_name\"] = limited_events_df[\"destination_in\"].map(destination_names)\n",
    "    limited_events_df[\"destination_out_name\"] = limited_events_df[\"destination_out\"].map(destination_names)\n",
    "    limited_events_df[\"out_exchange_name\"] = limited_events_df[\"destination_out\"].map(exchange_names)\n",
    "    limited_events_df[\"in_exchange_name\"] = limited_events_df[\"destination_in\"].map(exchange_names)\n",
    "    limited_events_df[\"pool_in\"] = limited_events_df[\"destination_in\"].map(pool_addresses)\n",
    "    limited_events_df[\"pool_out\"] = limited_events_df[\"destination_out\"].map(pool_addresses)\n",
    "\n",
    "    # Join limited_events_df with plans on rebalance_file_path = file_name\n",
    "    limited_events_df = limited_events_df.merge(plans, left_on=\"rebalance_file_path\", right_on=\"file_name\", how=\"left\")\n",
    "\n",
    "    return limited_events_df\n",
    "\n",
    "\n",
    "def load_data(autopool: AutopoolConstants):\n",
    "    destinations = get_full_table_as_df(Destinations, where_clause=Destinations.chain_id == autopool.chain.chain_id)\n",
    "    autopool_destinations = get_full_table_as_df(\n",
    "        AutopoolDestinations, where_clause=AutopoolDestinations.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    # 2 min to fetch\n",
    "    destination_states = get_full_table_as_df_with_block(\n",
    "        DestinationStates,\n",
    "        where_clause=DestinationStates.destination_vault_address.in_(\n",
    "            destinations[\"destination_vault_address\"].tolist()\n",
    "        ),\n",
    "    )\n",
    "    plans = get_full_table_as_df(\n",
    "        RebalancePlans, where_clause=RebalancePlans.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    events = get_full_table_as_df_with_tx_hash(\n",
    "        RebalanceEvents, where_clause=RebalanceEvents.autopool_vault_address == autopool.autopool_eth_addr\n",
    "    )\n",
    "    mainnet_blocks = get_full_table_as_df(Blocks, where_clause=Blocks.chain_id == autopool.chain.chain_id).sort_values(\n",
    "        \"block\"\n",
    "    )\n",
    "\n",
    "    limited_events_df = _extract_limited_events_data(autopool, events, plans, destination_states, destinations)\n",
    "    return destinations, autopool_destinations, destination_states, plans, events, mainnet_blocks, limited_events_df\n",
    "\n",
    "\n",
    "VP_METHODS = [\n",
    "    (\"getRate\", [\"getRate()(uint256)\"], None),\n",
    "    (\"get_virtual_price\", [\"get_virtual_price()(uint256)\"], None),\n",
    "    (\"convertToAssets_1e18\", [\"convertToAssets(uint256)(uint256)\", int(10**18)], int(10**18)),\n",
    "]\n",
    "\n",
    "\n",
    "def build_vp_calls(pool_address: str):\n",
    "    calls = []\n",
    "    for suffix, fn, _ in VP_METHODS:\n",
    "        key = f\"{pool_address}:{suffix}\"\n",
    "        calls.append(\n",
    "            Call(\n",
    "                target=pool_address,\n",
    "                function=fn,\n",
    "                returns=[(key, safe_normalize_with_bool_success)],\n",
    "            )\n",
    "        )\n",
    "    return calls\n",
    "\n",
    "\n",
    "def _get_working_virtual_price_column(df: pd.DataFrame, cols_in_priority: list[str]) -> pd.Series:\n",
    "    for col in cols_in_priority:\n",
    "        if not any(df[col].isna()):\n",
    "            return df[col]\n",
    "    print(df[cols_in_priority])\n",
    "\n",
    "    raise ValueError(\"could not identify working virtual price column\")\n",
    "\n",
    "\n",
    "def compute_apr(vp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    t0 = vp_df.index[0]\n",
    "    days = (vp_df.index - t0).total_seconds() / 86400.0\n",
    "\n",
    "    out0 = vp_df[\"out_vp\"].iloc[0]\n",
    "    in0 = vp_df[\"in_vp\"].iloc[0]\n",
    "\n",
    "    # annualized % using actual elapsed days; guard day=0 at start\n",
    "    vp_df[\"out_ann_pct\"] = np.where(days > 0, ((vp_df[\"out_vp\"] / out0) ** (365.0 / days) - 1.0), np.nan)\n",
    "    vp_df[\"in_ann_pct\"] = np.where(days > 0, ((vp_df[\"in_vp\"] / in0) ** (365.0 / days) - 1.0), np.nan)\n",
    "\n",
    "    return vp_df[[\"block\", \"out_vp\", \"in_vp\", \"out_ann_pct\", \"in_ann_pct\"]]\n",
    "\n",
    "\n",
    "def _fetch_vp_df(blocks_to_query: list[int], row: pd.Series, autopool: AutopoolConstants) -> pd.DataFrame:\n",
    "    out_addr = row[\"pool_out\"]\n",
    "    in_addr = row[\"pool_in\"]\n",
    "\n",
    "    calls_to_make = []\n",
    "    calls_to_make += build_vp_calls(out_addr)\n",
    "    calls_to_make += build_vp_calls(in_addr)\n",
    "\n",
    "    vp_df = get_raw_state_by_blocks(\n",
    "        calls_to_make,\n",
    "        blocks_to_query,\n",
    "        autopool.chain,\n",
    "        include_block_number=True,\n",
    "    )\n",
    "\n",
    "    # Coalesce per destination in the same priority order as VP_METHODS\n",
    "    out_cols = [f\"{out_addr}:{suffix}\" for suffix, _, _ in VP_METHODS]\n",
    "    in_cols = [f\"{in_addr}:{suffix}\" for suffix, _, _ in VP_METHODS]\n",
    "\n",
    "    vp_df[\"out_vp\"] = _get_working_virtual_price_column(vp_df, out_cols)\n",
    "    vp_df[\"in_vp\"] = _get_working_virtual_price_column(vp_df, in_cols)\n",
    "\n",
    "    apr_df = compute_apr(vp_df)\n",
    "    return apr_df\n",
    "\n",
    "\n",
    "def determine_forward_looking_vp(autopool: AutopoolConstants, row: pd.Series):\n",
    "    start_block = int(row[\"block\"])\n",
    "    chain_to_approx_blocks_per_day = {\n",
    "        ETH_CHAIN: 7150,  # crude approx\n",
    "        BASE_CHAIN: 43200,  # crude approx\n",
    "    }\n",
    "    approx_blocks_per_day = chain_to_approx_blocks_per_day[autopool.chain]\n",
    "\n",
    "    block_30_days = start_block + (approx_blocks_per_day * 30)\n",
    "    block_60_days = start_block + (approx_blocks_per_day * 60)\n",
    "\n",
    "    today_block = autopool.chain.get_block_near_top()\n",
    "\n",
    "    if (block_30_days > today_block) or (block_60_days > today_block):\n",
    "        return {\n",
    "            **row,\n",
    "            \"valid\": False,\n",
    "        }\n",
    "\n",
    "    blocks_to_query = [start_block, block_30_days, block_60_days]\n",
    "\n",
    "    apr_df = _fetch_vp_df(blocks_to_query, row, autopool)\n",
    "\n",
    "    actual_30_day_fee_and_base_out = apr_df.loc[apr_df[\"block\"] == block_30_days, \"out_ann_pct\"].values[0]\n",
    "    actual_60_day_fee_and_base_out = apr_df.loc[apr_df[\"block\"] == block_60_days, \"out_ann_pct\"].values[0]\n",
    "\n",
    "    actual_30_day_fee_and_base_in = apr_df.loc[apr_df[\"block\"] == block_30_days, \"in_ann_pct\"].values[0]\n",
    "    actual_60_day_fee_and_base_in = apr_df.loc[apr_df[\"block\"] == block_60_days, \"in_ann_pct\"].values[0]\n",
    "\n",
    "    block_timstamp_30_days = apr_df[\"block\"].loc[apr_df[\"block\"] == block_30_days].index[0]\n",
    "    block_timstamp_60_days = apr_df[\"block\"].loc[apr_df[\"block\"] == block_60_days].index[0]\n",
    "\n",
    "    return {\n",
    "        **row,\n",
    "        \"actual_30_day_fee_and_base_out\": actual_30_day_fee_and_base_out,\n",
    "        \"actual_60_day_fee_and_base_out\": actual_60_day_fee_and_base_out,\n",
    "        \"actual_30_day_fee_and_base_in\": actual_30_day_fee_and_base_in,\n",
    "        \"actual_60_day_fee_and_base_in\": actual_60_day_fee_and_base_in,\n",
    "        \"start_block\": start_block,\n",
    "        \"block_30_days\": block_30_days,\n",
    "        \"block_60_days\": block_60_days,\n",
    "        \"timestamp_30_days\": block_timstamp_30_days,\n",
    "        \"timestamp_60_days\": block_timstamp_60_days,\n",
    "        \"valid\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def add_virtual_price_values_no_threads(autopool: AutopoolConstants, limited_events_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_rows = limited_events_df.apply(lambda row: determine_forward_looking_vp(autopool, row), axis=1).tolist()\n",
    "    all_results_df = pd.DataFrame.from_records(new_rows)\n",
    "\n",
    "    all_results_df.loc[\n",
    "        all_results_df[\"destination_out\"] == autopool.autopool_eth_addr,\n",
    "        [\"actual_30_day_fee_and_base_out\", \"actual_60_day_fee_and_base_out\"],\n",
    "    ] = 0\n",
    "    all_results_df.loc[\n",
    "        all_results_df[\"destination_in\"] == autopool.autopool_eth_addr,\n",
    "        [\"actual_30_day_fee_and_base_in\", \"actual_60_day_fee_and_base_in\"],\n",
    "    ] = 0\n",
    "\n",
    "    return all_results_df\n",
    "\n",
    "\n",
    "for autopool in ALL_AUTOPOOLS:\n",
    "    if autopool.chain in [ETH_CHAIN, BASE_CHAIN]:\n",
    "        destinations, autopool_destinations, destination_states, plans, events, mainnet_blocks, limited_events_df = (\n",
    "            load_data(autopool)\n",
    "        )\n",
    "        full_df = add_virtual_price_values_no_threads(autopool, limited_events_df.reset_index())\n",
    "        df = full_df[full_df[\"valid\"] == True]\n",
    "        timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{autopool.name}_fee_and_base_apr_{timestamp_str}.csv\"\n",
    "        full_df.to_csv(filename)\n",
    "        print(df[\"valid\"].value_counts())\n",
    "        print(f\"Loaded {full_df.shape} rebalance events for {autopool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby(\"destination_out_name\")[\"valid\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for both 30-day and 60-day comparisons\n",
    "for period in [\"30_day\", \"60_day\"]:\n",
    "    # Plot for \"in\" destinations\n",
    "    fig_in = px.scatter(\n",
    "        df,\n",
    "        x=f\"fee_and_base_in\",\n",
    "        y=f\"actual_{period}_fee_and_base_in\",\n",
    "        color=\"destination_in_name\",\n",
    "        title=f\"Expected vs Actual {period.replace('_', '-').title()} Fee+Base APR (In)\",\n",
    "    )\n",
    "    fig_in.add_trace(\n",
    "        px.line(x=[0, 0.1], y=[0, 0.1]).data[0].update(line=dict(dash=\"dash\", color=\"gray\"), showlegend=False)\n",
    "    )\n",
    "    fig_in.show()\n",
    "    continue\n",
    "\n",
    "    # Plot for \"out\" destinations (only if fee_and_base_out exists)\n",
    "    df_with_out = df.dropna(subset=[\"fee_and_base_out\"])\n",
    "    if len(df_with_out) > 0:\n",
    "        fig_out = px.scatter(\n",
    "            df_with_out,\n",
    "            x=f\"fee_and_base_out\",\n",
    "            y=f\"actual_{period}_fee_and_base_out\",\n",
    "            color=\"destination_out_name\",\n",
    "            title=f\"Expected vs Actual {period.replace('_', '-').title()} Fee+Base APR (Out)\",\n",
    "        )\n",
    "        fig_out.add_trace(\n",
    "            px.line(x=[0, 0.1], y=[0, 0.1]).data[0].update(line=dict(dash=\"dash\", color=\"gray\"), showlegend=False)\n",
    "        )\n",
    "        fig_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one limitation could be, we can only predict at the .1% level, no way we are accurate closer than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Determine common x-axis range\n",
    "x_min = min(all_results_df[\"actual_30_day_fee_and_base_in\"].min(), all_results_df[\"fee_and_base_in\"].min())\n",
    "x_max = max(all_results_df[\"actual_30_day_fee_and_base_in\"].max(), all_results_df[\"fee_and_base_in\"].max())\n",
    "\n",
    "# Create subplots with 2 rows and 1 column\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Distribution of actual fee + base APR we enter\",\n",
    "        \"Distribution of expected fee + base APR we enter\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create histograms\n",
    "fig1 = px.histogram(all_results_df, x=\"actual_30_day_fee_and_base_in\")\n",
    "fig2 = px.histogram(all_results_df, x=\"fee_and_base_in\")\n",
    "\n",
    "# Add traces\n",
    "for trace in fig1.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in fig2.data:\n",
    "    fig.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# Update x-axes to have the same range\n",
    "fig.update_xaxes(range=[x_min, x_max], row=1, col=1)\n",
    "fig.update_xaxes(range=[x_min, x_max], row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=700, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = all_results_df.copy().dropna(subset=[\"actual_30_day_fee_and_base_in\"])\n",
    "sub_df[\"difference\"] = sub_df[\"actual_30_day_fee_and_base_in\"] - sub_df[\"fee_and_base_in\"]\n",
    "sub_df = sub_df.sort_values(\"difference\", ascending=False)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.ecdf(\n",
    "    all_results_df.dropna(subset=[\"actual_30_day_fee_and_base_in\"]),\n",
    "    x=[\"actual_30_day_fee_and_base_in\", \"actual_60_day_fee_and_base_in\", \"fee_and_base_in\"],\n",
    "    title=\"ECDF of actual fee + base APR we enter\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip to 5%, prevent the worst outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Expected Fee+Base APR (In)\", \"Actual 30-Day Fee+Base APR (In)\"))\n",
    "\n",
    "# ECDF for expected (fee_and_base_in)\n",
    "plot_df_expected = all_results_df.dropna(subset=[\"fee_and_base_in\"]).copy()\n",
    "fig_expected = px.ecdf(\n",
    "    plot_df_expected,\n",
    "    x=\"fee_and_base_in\",\n",
    ")\n",
    "for trace in fig_expected.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# ECDF for actual 30-day (actual_30_day_fee_and_base_in)\n",
    "plot_df_actual = all_results_df.dropna(subset=[\"actual_30_day_fee_and_base_in\"]).copy()\n",
    "fig_actual = px.ecdf(plot_df_actual, x=\"actual_30_day_fee_and_base_in\")\n",
    "for trace in fig_actual.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Fee+Base APR\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(title_text=\"Expected vs Actual Fee+Base APR Distribution (In)\", height=500, showlegend=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df[\"30_day_out_diff\"] = (\n",
    "    all_results_df[\"actual_30_day_fee_and_base_out\"] - all_results_df[\"fee_and_base_out\"]\n",
    ")\n",
    "all_results_df[\"60_day_out_diff\"] = (\n",
    "    all_results_df[\"actual_60_day_fee_and_base_out\"] - all_results_df[\"fee_and_base_out\"]\n",
    ")\n",
    "all_results_df[\"30_day_in_diff\"] = all_results_df[\"actual_30_day_fee_and_base_in\"] - all_results_df[\"fee_and_base_in\"]\n",
    "all_results_df[\"60_day_in_diff\"] = all_results_df[\"actual_60_day_fee_and_base_in\"] - all_results_df[\"fee_and_base_in\"]\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "plot_df = all_results_df.dropna(subset=[\"destination_out_name\", \"30_day_out_diff\"]).copy()\n",
    "\n",
    "fig = px.ecdf(\n",
    "    plot_df,\n",
    "    x=\"30_day_out_diff\",\n",
    "    color=\"destination_out_name\",  # one ECDF line per destination\n",
    "    title=\"Difference between Actual and Planned 30-Day Fee+Base APR (Out)\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"Destination (out)\",\n",
    ")\n",
    "print(\"diff >0 means actual > expected\")\n",
    "print(\"diff <0 means actual < expected\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = all_results_df.dropna(subset=[\"destination_in_name\", \"30_day_in_diff\"]).copy()\n",
    "\n",
    "fig = px.ecdf(\n",
    "    plot_df,\n",
    "    x=\"30_day_in_diff\",\n",
    "    color=\"destination_in_name\",\n",
    "    title=\"Difference between Actual and Planned 30-Day Fee+Base APR (In)\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"Destination (in)\",\n",
    ")\n",
    "print(\"diff >0 means actual > expected\")\n",
    "print(\"diff <0 means actual < expected\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should be doing the lowest hanging fruit first? like the ones that are way off?\n",
    "# Calculate absolute differences for sorting\n",
    "all_results_df[\"abs_30_day_out_diff\"] = all_results_df[\"30_day_out_diff\"].abs()\n",
    "all_results_df[\"abs_60_day_out_diff\"] = all_results_df[\"60_day_out_diff\"].abs()\n",
    "all_results_df[\"abs_30_day_in_diff\"] = all_results_df[\"30_day_in_diff\"].abs()\n",
    "all_results_df[\"abs_60_day_in_diff\"] = all_results_df[\"60_day_in_diff\"].abs()\n",
    "\n",
    "# Find the most off predictions for each category\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORST PREDICTIONS (60-day OUT)\")\n",
    "print(\"=\" * 80)\n",
    "all_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols = [\n",
    "    \"destination_in_name\",\n",
    "    \"destination_out_name\",\n",
    "    \"actual_30_day_fee_and_base_out\",\n",
    "    \"actual_60_day_fee_and_base_out\",\n",
    "    \"actual_30_day_fee_and_base_in\",\n",
    "    \"actual_60_day_fee_and_base_in\",\n",
    "    \"fee_and_base_out\",\n",
    "    \"fee_and_base_in\",\n",
    "    \"30_day_out_diff\",\n",
    "    \"60_day_out_diff\",\n",
    "    \"30_day_in_diff\",\n",
    "    \"60_day_in_diff\",\n",
    "    \"abs_30_day_out_diff\",\n",
    "    \"block\",\n",
    "    \"safe_value_out\",\n",
    "]\n",
    "\n",
    "all_results_df.sort_values(by=\"abs_60_day_out_diff\", ascending=False)[interesting_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainnet-launch-FtycU18g-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
